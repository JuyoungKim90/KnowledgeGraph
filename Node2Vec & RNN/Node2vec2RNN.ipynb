{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added to use driver\n",
    "import csv\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import operator\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from LSTMTagger import LSTMTagger\n",
    "from LSTMTrain import LSTMTrain\n",
    "from LSTMEvaluation import LSTMEvaluation\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_file = r'rnndata\\auxilary-mapping.txt'\n",
    "rating_file = r'rnndata\\rating.txt'\n",
    "train_file = r'rnndata/training.txt'\n",
    "test_file = r'rnndata/test.txt'\n",
    "negative_file = r'rnndata/negative.txt'\n",
    "pre_train_user_embedding = r'emb/node2vecuser.emb'\n",
    "pre_train_movie_embedding = r'emb/node2vecmov.emb'\n",
    "results_file = r'rnndata/results.txt'\n",
    "positive_path = r'rnndata/positive-path.txt'\n",
    "negative_path = r'rnndata/negative-path.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need to run again as  I have exported embedidnga nd model \n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "def createembedding(file, query):\n",
    "    print('Create the edge list')\n",
    "    with driver.session() as session, open(file, \"w\") as edges_file:\n",
    "        result = session.run(query)\n",
    "\n",
    "        writer = csv.writer(edges_file, delimiter=\" \")\n",
    "\n",
    "        for row in result:\n",
    "            writer.writerow([row[\"source\"], row[\"target\"]])\n",
    "    \n",
    "def createNode2Vec(file, EMBEDDING_FILENAME, EMBEDDING_MODEL_FILENAME):   \n",
    "    # Create a graph\n",
    "    graph=nx.read_edgelist(file, create_using = nx.DiGraph(), nodetype = None, data = [('weight', int)])\n",
    "\n",
    "    # Precompute probabilities and generate walks\n",
    "    node2vec = Node2Vec(graph, dimensions=10, walk_length=3, num_walks=20, workers=4)\n",
    "\n",
    "    ## if d_graph is big enough to fit in the memory, pass temp_folder which has enough disk space\n",
    "    # Note: It will trigger \"sharedmem\" in Parallel, which will be slow on smaller graphs\n",
    "    #node2vec = Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=4, temp_folder=\"/mnt/tmp_data\")\n",
    "    # Embed\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `diemnsions` and `workers` are automatically passed (from the Node2Vec constructor)\n",
    "\n",
    "    # Look for most similar nodes\n",
    "    #model.wv.most_similar('2')  # Output node names are always strings\n",
    "\n",
    "    # Save embeddings for later use\n",
    "    model.wv.save_word2vec_format(EMBEDDING_FILENAME)\n",
    "\n",
    "    # Save model for later use\n",
    "    model.save(EMBEDDING_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create the edge list\n"
     ]
    }
   ],
   "source": [
    "#FILES\n",
    "file = 'graph/movies.edgelist'\n",
    "EMBEDDING_FILENAME = 'emb/node2vecmov.emb'\n",
    "EMBEDDING_MODEL_FILENAME = 'emb/embdnode2vecmov.model'\n",
    "movienode2vecquery = \"\"\"\\\n",
    "        MATCH (m:Movie)--(other)\n",
    "        RETURN id(m) AS source, id(other) AS target\n",
    "        \"\"\"\n",
    "\n",
    "createembedding(file, movienode2vecquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|███████████████████████████████████████| 9980/9980 [00:00<00:00, 10788.50it/s]\n",
      "C:\\Users\\meetn\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "createNode2Vec(file, EMBEDDING_FILENAME, EMBEDDING_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create the edge list\n"
     ]
    }
   ],
   "source": [
    "file = 'graph/users.edgelist'\n",
    "EMBEDDING_FILENAME = 'emb/node2vecuser.emb'\n",
    "EMBEDDING_MODEL_FILENAME = 'emb/embdnode2vecuser.model'\n",
    "usernode2vecquery = \"\"\"\\\n",
    "        MATCH (u:User)--(other)\n",
    "        RETURN id(u) AS source, id(other) AS target\n",
    "        \"\"\"\n",
    "createembedding(file, usernode2vecquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|███████████████████████████████████████| 6257/6257 [00:00<00:00, 26249.72it/s]\n",
      "C:\\Users\\meetn\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "createNode2Vec(file, EMBEDDING_FILENAME, EMBEDDING_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120828"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is used to map the auxiliary information (genre) into mapping ID for MovieLens\n",
    "genrefile = \"\"\"\\\n",
    "        MATCH (m:Movie)<-[:IS_GENRE_OF]-(g:Genre)\n",
    "        RETURN id(m), collect(id(g))\n",
    "        \"\"\"\n",
    "with driver.session() as session:\n",
    "    result = session.run(genrefile)\n",
    "df = pd.DataFrame([dict(row) for row in result])\n",
    "df.to_csv(aux_file, header=None, index=None, sep='|', mode='a')\n",
    "string = open(aux_file).read()\n",
    "new_str = re.sub('[\\\\[\\\\]]', '', string)\n",
    "open(aux_file, 'w').write(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingfile = \"\"\"\\\n",
    "        MATCH (u:User)-[r:RATED]->(m:Movie)\n",
    "        RETURN id(u), id(m), r.rating, r.timestamp\n",
    "        \"\"\"\n",
    "with driver.session() as session:\n",
    "    result = session.run(ratingfile)\n",
    "df = pd.DataFrame([dict(row) for row in result])\n",
    "df.to_csv(rating_file, header=None, index=None, sep='\\t', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is used to split the user-movie interaction data into traning and test according to the timestamp\n",
    "#load the user-item rating data with the timestamp\n",
    "def load_data_rating(rating_file):\n",
    "    fr_rating = open(rating_file,'r')\n",
    "    rating_data = {}\n",
    "    for line in fr_rating:\n",
    "        lines = line.split('\\t')\n",
    "        \n",
    "        user = lines[0]\n",
    "        item = lines[1]\n",
    "        time = lines[3].replace('\\n', '')\n",
    "    \n",
    "        item_list = []\n",
    "\n",
    "        if user in rating_data:\n",
    "            rating_data[user].update({item:time})            \n",
    "        else:\n",
    "            rating_data.update({user:{item:time}})\n",
    "    return rating_data\n",
    "\n",
    "#split rating_rating data into training and test data by timestamp    \n",
    "def split_rating_into_train_test(rating_data, train_file, test_file, ratio):  \n",
    "    fw_train = open(train_file,'w')\n",
    "    fw_test = open(test_file,'w')\n",
    "    for user in rating_data:\n",
    "        item_list = rating_data[user]\n",
    "\n",
    "        sorted_u = sorted(item_list.items(), key=operator.itemgetter(1))\n",
    "        sorted_u = dict(sorted_u)\n",
    "        \n",
    "        rating_num = rating_data[user].__len__()\n",
    "        train_size = int(round(rating_num*ratio, 0))# round_int (rating_num, ratio)\n",
    "\n",
    "        flag = 0\n",
    "\n",
    "        for item in sorted_u:\n",
    "\n",
    "            if flag < train_size:\n",
    "                line = user+'\\t'+ item + '\\t' + sorted_u[item]+'\\n'\n",
    "                fw_train.write(line)\n",
    "                flag = flag + 1\n",
    "            else:\n",
    "                line = user+'\\t' + item + '\\t' +sorted_u[item]+'\\n'\n",
    "                fw_test.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_data = load_data_rating(rating_file)\n",
    "split_rating_into_train_test(rating_data, train_file, test_file, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is used to sample negative movies that user has not interactions with, so as to balance model training process\n",
    "#load training data\n",
    "def load_data_train(file):\n",
    "    train_dict = {}\n",
    "    all_movie_list = []\n",
    "\n",
    "    for line in file:\n",
    "        lines = line.split('\\t')\n",
    "        user = lines[0]\n",
    "        movie = lines[1].replace('\\n','')\n",
    "        \n",
    "        if user not in train_dict:\n",
    "            init_movie_list = []\n",
    "            init_movie_list.append(movie)\n",
    "            train_dict.update({user:init_movie_list})\n",
    "        else:\n",
    "            train_dict[user].append(movie)\n",
    "        \n",
    "        if movie not in all_movie_list:\n",
    "            all_movie_list.append(movie)\n",
    "\n",
    "    return train_dict, all_movie_list\n",
    "\n",
    "#sample negative movies for all users in training data\n",
    "def negative_sample(train_dict, all_movie_list, shrink, fw_negative):\n",
    "    all_movie_size = len(all_movie_list)\n",
    "\n",
    "    for user in train_dict:\n",
    "        user_train_movie = train_dict[user]\n",
    "        user_train_movie_size = len(user_train_movie)\n",
    "        negative_size = math.ceil(user_train_movie_size * shrink)\n",
    "        user_negative_movie = []\n",
    "\n",
    "        while (len(user_negative_movie) < negative_size):\n",
    "            negative_index = randint(0, (all_movie_size - 1))\n",
    "            negative_movie = str(all_movie_list[negative_index])\n",
    "            if negative_movie not in user_train_movie and negative_movie not in user_negative_movie:\n",
    "                user_negative_movie.append(negative_movie)\n",
    "                line = user + '\\t' + negative_movie + '\\n'\n",
    "                fw_negative.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_train = open(train_file,'r')\n",
    "fw_negative = open(negative_file,'w')\n",
    "train_dict, all_movie_list = load_data_train(fr_train)\n",
    "negative_sample(train_dict, all_movie_list, 0.05, fw_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build knowledge graph and mine the connected paths between users and movies in the training data of MovieLens\n",
    "#load training (positive) or negative user-movie interaction data\n",
    "def load_data(file):\n",
    "    data = []\n",
    "    #print(file)\n",
    "    for line in file:\n",
    "        lines = line.split('\\t')\n",
    "        user = lines[0]\n",
    "        movie = lines[1].replace('\\n','')\n",
    "        data.append((user, movie))\n",
    "\n",
    "    return data\n",
    "\n",
    "#add user-movie interaction data into the graph\n",
    "def add_user_movie_interaction_into_graph(positive_rating):\n",
    "    Graph = nx.DiGraph()       \n",
    "\n",
    "    for pair in positive_rating:\n",
    "        user = pair[0]\n",
    "        movie = pair[1]\n",
    "        user_node = 'u' + user\n",
    "        movie_node = 'i' + movie\n",
    "        Graph.add_node(user_node)\n",
    "        Graph.add_node(movie_node)\n",
    "        Graph.add_edge(user_node, movie_node)\n",
    "\n",
    "    return Graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add auxiliary information (e.g., actor, director, genre) into graph\n",
    "def add_auxiliary_into_graph(fr_auxiliary, Graph):\n",
    "    for line in fr_auxiliary:\n",
    "        lines = line.replace('\\n', '').split('|')\n",
    "        if len(lines) != 2: continue\n",
    "        \n",
    "        movie_id = lines[0]\n",
    "        genre_list = lines[1].split(',')\n",
    "        #director_list = lines[2].split(',')\n",
    "        #actor_list = lines[3].split(',')\n",
    "\n",
    "        #add movie nodes into Graph, in case the movie is not included in the training data\n",
    "        movie_node = 'i' + movie_id\n",
    "        if not Graph.has_node(movie_node):\n",
    "            Graph.add_node(movie_node)\n",
    "\n",
    "        #add the genre nodes into the graph;  \n",
    "        #as genre connection is too dense, we add one genre to avoid over-emphasizing its effect\n",
    "        genre_id = genre_list[0]\n",
    "        genre_node = 'g' + genre_id\n",
    "        if not Graph.has_node(genre_node):\n",
    "            Graph.add_node(genre_node)\n",
    "        Graph.add_edge(movie_node, genre_node)\n",
    "        Graph.add_edge(genre_node, movie_node)\n",
    "\n",
    "        #add the director nodes into the graph\n",
    "        '''for director_id in director_list:\n",
    "            director_node = 'd' + director_id\n",
    "            if not Graph.has_node(director_node):\n",
    "                Graph.add_node(director_node)\n",
    "            Graph.add_edge(movie_node, director_node)\n",
    "            Graph.add_edge(director_node, movie_node)\n",
    "\n",
    "        #add the actor nodes into the graph\n",
    "        for actor_id in actor_list:\n",
    "            actor_node = 'a' + actor_id\n",
    "            if not Graph.has_node(actor_node):\n",
    "                Graph.add_node(actor_node)\n",
    "            Graph.add_edge(movie_node, actor_node)\n",
    "            Graph.add_edge(actor_node, movie_node)'''\n",
    "\n",
    "    return Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mine qualified paths between user and movie nodes, and get sampled paths between nodes\n",
    "def mine_paths_between_nodes(Graph, user_node, movie_node, maxLen, sample_size, fw_file):\n",
    "    connected_path = [] \n",
    "    for path in nx.all_simple_paths(Graph, source=user_node, target=movie_node, cutoff=maxLen):\n",
    "        #if len(path) >= 4:\n",
    "             #print(len(path))\n",
    "        if len(path) == maxLen + 1: \n",
    "            connected_path.append(path)\n",
    "\n",
    "    path_size = len(connected_path)\n",
    "   \n",
    "    #as there is a huge number of paths connected user-movie nodes, we get randomly sampled paths\n",
    "    #random sample can better balance the data distribution and model complexity\n",
    "    if path_size > sample_size:\n",
    "        random.shuffle(connected_path)\n",
    "        connected_path = connected_path[:sample_size]\n",
    "    \n",
    "    for path in connected_path:\n",
    "        #print(path)\n",
    "        line = \",\".join(path) + '\\n'\n",
    "        fw_file.write(line)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump the postive or negative paths \n",
    "def dump_paths(Graph, rating_pair, maxLen, sample_size, fw_file):\n",
    "    for pair in rating_pair:\n",
    "        #print(pair)\n",
    "        user_id = pair[0]\n",
    "        movie_id = pair[1]\n",
    "        user_node = 'u' + user_id\n",
    "        movie_node = 'i' + movie_id\n",
    "\n",
    "        if Graph.has_node(user_node) and Graph.has_node(movie_node):\n",
    "            mine_paths_between_nodes(Graph, user_node, movie_node, maxLen, sample_size, fw_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of user-movie interaction data is:  28642 \n",
      "\n",
      "The number of negative sampled data is:  1508 \n",
      "\n",
      "The knowledge graph has been built completely \n",
      "\n",
      "The number of nodes is:  9969 \n",
      "\n",
      "The number of edges is  48050 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fr_training = open(train_file,'r')\n",
    "fr_negative = open(negative_file, 'r')\n",
    "fr_auxiliary = open(aux_file,'r')\n",
    "fw_positive_path = open(positive_path, 'w')\n",
    "fw_negative_path = open(negative_path, 'w')\n",
    "positive_rating = load_data(fr_training)\n",
    "negative_rating = load_data(fr_negative)\n",
    "print('The number of user-movie interaction data is:  ' + str(len(positive_rating))+ ' \\n')\n",
    "print('The number of negative sampled data is:  ' + str(len(negative_rating))+ ' \\n') \n",
    "\n",
    "Graph = add_user_movie_interaction_into_graph(positive_rating)\n",
    "Graph = add_auxiliary_into_graph(fr_auxiliary, Graph)\n",
    "print('The knowledge graph has been built completely \\n')\n",
    "print('The number of nodes is:  ' + str(len(Graph.nodes()))+ ' \\n') \n",
    "print('The number of edges is  ' + str(len(Graph.edges()))+ ' \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_paths(Graph, positive_rating, 3, 5, fw_positive_path)\n",
    "dump_paths(Graph, negative_rating, 3, 5, fw_negative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=10\n",
    "hidden_dim=16\n",
    "out_dim=1\n",
    "learning_rate=0.2\n",
    "\n",
    "fr_postive = open(positive_path, 'r')\n",
    "fr_negative = open(negative_path, 'r')\n",
    "fr_pre_user = open(pre_train_user_embedding, 'r')\n",
    "fr_pre_movie = open(pre_train_movie_embedding, 'r')\n",
    "fr_train = open(train_file,'r')\n",
    "fr_test = open(test_file,'r')\n",
    "\n",
    "\n",
    "node_count = 0 #count the number of all entities (user, movie and attributes)\n",
    "all_variables = {} #save variable and corresponding id\n",
    "paths_between_pairs = {} #save all the paths (both positive and negative) between a user-movie pair\n",
    "positive_label = [] #save the positive user-movie pairs\n",
    "all_user = [] #save all the users\n",
    "all_movie = [] #save all the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is part is aims to feed connected paths into the recurrent neural network to train and test the proposed methods.\n",
    "#load postive or negative paths, map all nodes in paths into ids\n",
    "all_variables, paths_between_pairs, positive_label, all_user, all_movie\n",
    "def load_paths(fr_file, isPositive):    \n",
    "    node_count = 0\n",
    "    for line in fr_file:\n",
    "        #print(line)\n",
    "        line = line.replace('\\n', '')\n",
    "        lines = line.split(',')\n",
    "        user = lines[0]\n",
    "        movie = lines[-1]\n",
    "\n",
    "        if user not in all_user:\n",
    "            all_user.append(user)\n",
    "        if movie not in all_movie:\n",
    "            all_movie.append(movie)\n",
    "\n",
    "        key = (user, movie)\n",
    "        value = []\n",
    "        path = []\n",
    "\n",
    "        if isPositive:\n",
    "            if key not in positive_label:\n",
    "                positive_label.append(key)\n",
    "\n",
    "        for node in lines:\n",
    "            if node not in all_variables:\n",
    "                all_variables.update({node:node_count})\n",
    "                node_count = node_count + 1\n",
    "            path.append(node)\n",
    "\n",
    "        if key not in paths_between_pairs:\n",
    "            value.append(path)\n",
    "            paths_between_pairs.update({key:value})\n",
    "        else:\n",
    "            paths_between_pairs[key].append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pre-train-user or movie embeddings\n",
    "global pre_embedding\n",
    "def load_pre_embedding(fr_pre_file, isUser):\n",
    "    count = 0\n",
    "    for line in fr_pre_file:\n",
    "        if count != 0:            \n",
    "            lines = line.split(' ')\n",
    "            node = lines[0]\n",
    "            if isUser:\n",
    "                node = 'u' + node\n",
    "            else:\n",
    "                node = 'i' + node\n",
    "\n",
    "            if node in all_variables:\n",
    "                node_id = all_variables[node]\n",
    "                embedding = [float(x) for x in lines[1:]]\n",
    "                embedding = np.array(embedding)\n",
    "                pre_embedding[node_id] = embedding\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training or test data\n",
    "def load_data(fr_file):\n",
    "    data_dict = {}\n",
    "\n",
    "    for line in fr_file:\n",
    "            lines = line.replace('\\n', '').split('\\t')\n",
    "            user = 'u' + lines[0]\n",
    "            item = 'i' + lines[1]\n",
    "\n",
    "            if user not in data_dict:\n",
    "                data_dict.update({user:[item]})\n",
    "            elif item not in data_dict[user]:\n",
    "                data_dict[user].append(item)\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "load_paths(fr_postive, True)\n",
    "load_paths(fr_negative, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of all variables is :5277\n",
      "the duration for loading user path is 0:01:50.003111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('The number of all variables is :' + str(len(all_variables)))\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "print ('the duration for loading user path is ' + str(duration) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the duration for loading embedding is 0:00:00.040874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "node_size = len(all_variables)\n",
    "pre_embedding = np.random.rand(node_size, input_dim) #embeddings for all nodes\n",
    "load_pre_embedding(fr_pre_user, True)\n",
    "load_pre_embedding(fr_pre_movie, False)\n",
    "pre_embedding = torch.FloatTensor(pre_embedding)\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "print ('the duration for loading embedding is ' + str(duration) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meetn\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "model = LSTMTagger(node_size, input_dim, hidden_dim, out_dim, pre_embedding)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "#model = model.cpu()\n",
    "model_train = LSTMTrain(model, iteration, learning_rate, paths_between_pairs, positive_label, \\\n",
    "    all_variables, all_user, all_movie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meetn\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\meetn\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0]: loss is 17.291486152507787\n",
      "epoch[1]: loss is 63.668797023387015\n",
      "epoch[2]: loss is 65.12790837045941\n",
      "epoch[3]: loss is 66.74145469803989\n",
      "epoch[4]: loss is 64.6142257083593\n"
     ]
    }
   ],
   "source": [
    "#embedding_dict = model_train.train()\n",
    "all_user1, all_movie1, model_n = model_train.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meetn\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTMTagger. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\meetn\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\meetn\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\meetn\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model_n, 'rnnmodel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_n = torch.load('rnnmodel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump the post-train user or item embedding\n",
    "def dump_post_embedding(all_user1, all_movie1, model_n):\n",
    "    embedding_dict = {}\n",
    "    \n",
    "    node_list = all_user1 + all_movie1\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    #ur_id = self.model\n",
    "    for node in node_list:\n",
    "        node_id = torch.LongTensor([int(all_variables[node])])\n",
    "        node_id = Variable(node_id)\n",
    "        node_id = node_id.to(device)\n",
    "        #input_ids_tensor = input_ids_tensor.to(self.device)\n",
    "        \n",
    "        node_embedding = model_n.embedding(node_id).squeeze().cpu().data.numpy()\n",
    "        if node not in embedding_dict:\n",
    "            embedding_dict.update({node:node_embedding})\n",
    "\n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n = model_n.to(device)\n",
    "embedding_dict = dump_post_embedding(all_user1, all_movie1, model_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training finished\n",
      "the duration for model training is 0:30:00.201496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('model training finished')\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "print ('the duration for model training is ' + str(duration) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "train_dict = load_data(fr_train)\n",
    "test_dict = load_data(fr_test)\n",
    "model_evaluation = LSTMEvaluation(embedding_dict, all_movie, train_dict, test_dict)\n",
    "top_score_dict = model_evaluation.calculate_ranking_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(top_score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@1 is: 0.008264462809917356\n",
      "precision@5 is: 0.007438016528925619\n",
      "precision@10 is: 0.006611570247933886\n",
      "mrr@10 is: 0.022405876951331498\n",
      "the duration for model evaluation is 0:00:03.370471\n",
      "\n",
      "the duration for loading item embedding is 0:00:03.372466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if top_score_dict:\n",
    "    precision_1,_ = model_evaluation.calculate_results(top_score_dict, 1)\n",
    "    precision_5,_ = model_evaluation.calculate_results(top_score_dict, 5)\n",
    "    precision_10, mrr_10 = model_evaluation.calculate_results(top_score_dict, 10)\n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    print ('the duration for model evaluation is ' + str(duration) + '\\n')\n",
    "\n",
    "    #write_results(fw_results, precision_1, precision_5, precision_10, mrr_10)\n",
    "    fw_results = open(results_file, 'w')\n",
    "    line = 'precision@1: ' + str(precision_1) + '\\n' + 'precision@5: ' + str(precision_5) + '\\n' \\\n",
    "        + 'precision@10: ' + str(precision_10) + '\\n' + 'mrr: ' + str(mrr_10) + '\\n'\n",
    "    fw_results.write(line)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    print ('the duration for loading item embedding is ' + str(duration) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
